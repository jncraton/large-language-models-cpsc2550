Tokenization
============

Splitting Text
--------------

- Text is represented in our systems as a series of bits
- What natural boundaries do we have to split text?

Possible Splits
---------------

- Bits
- Characters
- Words
- Sentences

Challenges
----------

- Punctuation
- [Occurences in training set](https://iter.ca/post/gpt-crash/)
- Whitespace
- Diverse languages support (natural and programming)

OpenAI Tokenizer
----------------

- [Live Demo](https://platform.openai.com/tokenizer)
- Used for GPT3.5 and GPT 4